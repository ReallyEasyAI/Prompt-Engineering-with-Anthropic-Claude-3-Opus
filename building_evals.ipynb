{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Evals\n",
    "Optimizing Claude to give you the highest possible accuracy on a task is an empirical science, and a process of continuous improvement. Whether you are trying to know if a change to your prompt made the model perform better on a key metric, or whether you are trying to gauge if the model is good enough to launch into production, a good system for offline evaluation is critical to success.\n",
    "\n",
    "In this recipe, we will walk through common patterns in building evaluations, and useful rules of thumb to follow when doing so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parts of an Eval\n",
    "Evals typically have four parts.\n",
    "- An input prompt that is fed to the model. We will ask Claude to generate a completion based on this prompt. Often when we design our evals the input column will contain a set of variable inputs that get fed into a prompt template at test time.\n",
    "- An output that comes from running the input prompt through the model we want to evaluate.\n",
    "- A \"golden answer\" to which we compare the model output. The golden answer could be a mandatory exact match, or it could be an example of a perfect answer meant to give a grader a point of comparison to base their scoring on.\n",
    "- A score, generated by one of the grading methods discussed below, that represents how the model did on the question.\n",
    "\n",
    "## Eval Grading Methods\n",
    "There are two things about evals that can be time consuming and expensive. The first is writing the questions and golden answers for the eval. The second is grading. Writing questions and golden answers can be quite time consuming if you do not have a dataset already available or a way to create one without manually generating questions (consider using Claude to generate your questions!), but has the benefit of typically being a one-time fixed cost. You write questions and golden answers, and very rarely have to re-write them. Grading on the other hand is a cost you will incur every time you re-run your eval, in perpetuity - and you will likely re-run your eval a lot. As a result, building evals that can be quickly and cheaply graded should be at the center of your design choices.\n",
    "\n",
    "There are three common ways to grade evals.\n",
    "- **Code-based grading:** This involves using standard code (mostly string matching and regular expressions) to grade the model's outputs. Common versions are checking for an exact match against an answer, or checking that a string contains some key phrase(s). This is by far the best grading method if you can design an eval that allows for it, as it is super fast and highly reliable. However, many evaluations do not allow for this style of grading.\n",
    "- **Human grading:** A human looks at the model-generated answer, compares it to the golden answer, and assigns a score. This is the most capable grading method as it _can_ be used on almost any task, but it is also incredibly slow and expensive, particularly if you've built a large eval. You should mostly try to avoid designing evals that require human grading if you can help it.\n",
    "- **Model-based grading:** It turns out that Claude is highly capable of grading itself, and can be used to grade a wide variety of tasks that might have historically required humans, such as analysis of tone in creative writing or accuracy in free-form question answering. You do this by writing a _grader prompt_ for Claude.\n",
    "\n",
    "Let's walk through an example of each grading method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code-based Grading\n",
    "Here we will be grading an eval where we ask Claude to successfully identify how many legs something has. We want Claude to output just a number of legs, and we design the eval in a way that we can use an exact-match code-based grader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and read in required packages, plus create an anthropic client.\n",
    "%pip install anthropic pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic import Anthropic\n",
    "client = Anthropic()\n",
    "MODEL_NAME = \"claude-3-opus-20240229\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our input prompt template for the task.\n",
    "def build_input_prompt(animal_statement):\n",
    "    user_content = f\"\"\"You will be provided a statement about an animal and your job is to determine how many legs that animal has.\n",
    "    \n",
    "    Here is the animal statement.\n",
    "    <animal_statement>{animal_statement}</animal_statement>\n",
    "    \n",
    "    How many legs does the animal have? Return just the number of legs as an integer and nothing else.\"\"\"\n",
    "\n",
    "    messages = [{'role': 'user', 'content': user_content}]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our eval (in practice you might do this as a jsonl or csv file instead).\n",
    "eval = [\n",
    "    {\n",
    "        \"animal_statement\": 'The animal is a human.',\n",
    "        \"golden_answer\": '2'\n",
    "    },\n",
    "        {\n",
    "        \"animal_statement\": 'The animal is a snake.',\n",
    "        \"golden_answer\": '0'\n",
    "    },\n",
    "        {\n",
    "        \"animal_statement\": 'The fox lost a leg, but then magically grew back the leg he lost and a mysterious extra leg on top of that.',\n",
    "        \"golden_answer\": '5'\n",
    "    },\n",
    "        {\n",
    "        \"animal_statement\": 'The animal is a spider.',\n",
    "        \"golden_answer\": '8'\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Animal Statement: The animal is a human.\n",
      "Golden Answer: 2\n",
      "Output: 2\n",
      "\n",
      "Animal Statement: The animal is a snake.\n",
      "Golden Answer: 0\n",
      "Output: 0\n",
      "\n",
      "Animal Statement: The fox lost a leg, but then magically grew back the leg he lost and a mysterious extra leg on top of that.\n",
      "Golden Answer: 5\n",
      "Output: 5\n",
      "\n",
      "Animal Statement: The animal is a spider.\n",
      "Golden Answer: 8\n",
      "Output: 8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get completions for each input.\n",
    "# Define our get_completion function (including the stop sequence discussed above).\n",
    "def get_completion(messages):\n",
    "    response = client.messages.create(\n",
    "        model=MODEL_NAME,\n",
    "        max_tokens=5,\n",
    "        messages=messages\n",
    "    )\n",
    "    return response.content[0].text\n",
    "\n",
    "# Get completions for each question in the eval.\n",
    "outputs = [get_completion(build_input_prompt(question['animal_statement'])) for question in eval]\n",
    "\n",
    "# Let's take a quick look at our outputs\n",
    "for output, question in zip(outputs, eval):\n",
    "    print(f\"Animal Statement: {question['animal_statement']}\\nGolden Answer: {question['golden_answer']}\\nOutput: {output}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# Check our completions against the golden answers.\n",
    "# Define a grader function\n",
    "def grade_completion(output, golden_answer):\n",
    "    return output == golden_answer\n",
    "\n",
    "# Run the grader function on our outputs and print the score.\n",
    "grades = [grade_completion(output, question['golden_answer']) for output, question in zip(outputs, eval)]\n",
    "print(f\"Score: {sum(grades)/len(grades)*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human grading\n",
    "Now let's imagine that we are grading an eval where we've asked Claude a series of open ended questions, maybe for a general purpose chat assistant. Unfortunately, answers could be varied and this can not be graded with code. One way we can do this is with human grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our input prompt template for the task.\n",
    "def build_input_prompt(question):\n",
    "    user_content = f\"\"\"Please answer the following question:\n",
    "    <question>{question}</question>\"\"\"\n",
    "\n",
    "    messages = [{'role': 'user', 'content': user_content}]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our eval. For this task, the best \"golden answer\" to give a human are instructions on what to look for in the model's output.\n",
    "eval = [\n",
    "    {\n",
    "        \"question\": 'Please design me a workout for today that features at least 50 reps of pulling leg exercises, at least 50 reps of pulling arm exercises, and ten minutes of core.',\n",
    "        \"golden_answer\": 'A correct answer should include a workout plan with 50 or more reps of pulling leg exercises (such as deadlifts, but not such as squats which are a pushing exercise), 50 or more reps of pulling arm exercises (such as rows, but not such as presses which are a pushing exercise), and ten minutes of core workouts. It can but does not have to include stretching or a dynamic warmup, but it cannot include any other meaningful exercises.'\n",
    "    },\n",
    "    {\n",
    "        \"question\": 'Send Jane an email asking her to meet me in front of the office at 9am to leave for the retreat.',\n",
    "        \"golden_answer\": 'A correct answer should decline to send the email since the assistant has no capabilities to send emails. It is okay to suggest a draft of the email, but not to attempt to send the email, call a function that sends the email, or ask for clarifying questions related to sending the email (such as which email address to send it to).'\n",
    "    },\n",
    "    {\n",
    "        \"question\": 'Who won the super bowl in 2024 and who did they beat?', # Claude should get this wrong since it comes after its training cutoff.\n",
    "        \"golden_answer\": 'A correct answer states that the Kansas City Chiefs defeated the San Francisco 49ers.'\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Please design me a workout for today that features at least 50 reps of pulling leg exercises, at least 50 reps of pulling arm exercises, and ten minutes of core.\n",
      "Golden Answer: A correct answer should include a workout plan with 50 or more reps of pulling leg exercises (such as deadlifts, but not such as squats which are a pushing exercise), 50 or more reps of pulling arm exercises (such as rows, but not such as presses which are a pushing exercise), and ten minutes of core workouts. It can but does not have to include stretching or a dynamic warmup, but it cannot include any other meaningful exercises.\n",
      "Output: Here's a workout plan for today that incorporates at least 50 reps of pulling leg exercises, at least 50 reps of pulling arm exercises, and ten minutes of core:\n",
      "\n",
      "Pulling Leg Exercises (50 reps total):\n",
      "1. Hamstring Curls: 3 sets of 12 reps (36 reps)\n",
      "2. Single-leg Deadlifts: 2 sets of 7 reps per leg (14 reps)\n",
      "\n",
      "Pulling Arm Exercises (50 reps total):\n",
      "1. Bent-over Rows: 3 sets of 10 reps (30 reps)\n",
      "2. Chin-ups or Assisted Chin-ups: 2 sets of 10 reps (20 reps)\n",
      "\n",
      "Core Exercises (10 minutes):\n",
      "1. Plank: 3 sets of 1 minute each\n",
      "2. Russian Twists: 3 sets of 20 reps (60 reps total)\n",
      "3. Bicycle Crunches: 3 sets of 20 reps (60 reps total)\n",
      "\n",
      "Workout Structure:\n",
      "1. Start with a 5-minute warm-up (light jogging, jumping jacks, or dynamic stretches).\n",
      "2. Perform the pulling leg exercises, resting for 60-90 seconds between sets.\n",
      "3. Move on to the pulling arm exercises, resting for 60-90 seconds between sets.\n",
      "4. Finish with the core exercises, performing each exercise for the prescribed number of sets and reps, with minimal rest between sets.\n",
      "5. End the workout with a 5-minute cool-down stretch, focusing on the muscle groups worked during the session.\n",
      "\n",
      "Remember to maintain proper form throughout the exercises, and adjust the weights or resistance as needed to suit your fitness level. Stay hydrated and listen to your body, resting when necessary.\n",
      "\n",
      "Question: Send Jane an email asking her to meet me in front of the office at 9am to leave for the retreat.\n",
      "Golden Answer: A correct answer should decline to send the email since the assistant has no capabilities to send emails. It is okay to suggest a draft of the email, but not to attempt to send the email, call a function that sends the email, or ask for clarifying questions related to sending the email (such as which email address to send it to).\n",
      "Output: I apologize, but I am not able to send emails on your behalf. As an AI language model, I do not have the capability to interact with email systems or send messages to individuals. \n",
      "\n",
      "If you need to send an email to Jane, you will need to use your own email account and compose the message yourself. A simple email could look something like this:\n",
      "\n",
      "Subject: Meeting for Retreat\n",
      "\n",
      "Hi Jane,\n",
      "\n",
      "Could you please meet me in front of the office at 9am on [date]? We will leave together for the retreat at that time.\n",
      "\n",
      "Thanks,\n",
      "[Your Name]\n",
      "\n",
      "Remember to replace [date] with the actual date of the retreat and [Your Name] with your own name. I hope this helps provide a template for your email to Jane.\n",
      "\n",
      "Question: Who won the super bowl in 2024 and who did they beat?\n",
      "Golden Answer: A correct answer states that the Kansas City Chiefs defeated the San Francisco 49ers.\n",
      "Output: I apologize, but as of February 2023, the Super Bowl for the year 2024 has not yet taken place. The Super Bowl is typically played in early February each year, so Super Bowl LVIII (58) is scheduled to be played on February 11, 2024, at Allegiant Stadium in Paradise, Nevada. The participating teams will be determined by the results of the 2023 NFL regular season and playoffs, which have not yet occurred. Therefore, it is impossible to know who will win Super Bowl LVIII or whom they will beat at this time.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get completions for each input.\n",
    "# Define our get_completion function (including the stop sequence discussed above).\n",
    "def get_completion(messages):\n",
    "    response = client.messages.create(\n",
    "        model=MODEL_NAME,\n",
    "        max_tokens=2048,\n",
    "        messages=messages\n",
    "    )\n",
    "    return response.content[0].text\n",
    "\n",
    "# Get completions for each question in the eval.\n",
    "outputs = [get_completion(build_input_prompt(question['question'])) for question in eval]\n",
    "\n",
    "# Let's take a quick look at our outputs\n",
    "for output, question in zip(outputs, eval):\n",
    "    print(f\"Question: {question['question']}\\nGolden Answer: {question['golden_answer']}\\nOutput: {output}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we will need to have a human grade this question, from here you would evaluate the outputs against the golden answers yourself, or write the outputs and golden answers to a csv and hand them to another human grader."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-based Grading\n",
    "Having to manually grade the above eval every time is going to get very annoying very fast, especially if the eval is a more realistic size (dozens, hundreds, or even thousands of questions). Luckily, there's a better way! We can actually have Claude do the grading for us. Let's take a look at how to do that using the same eval and completions from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Score: 66.66666666666666%\n",
      "Detailed Feedback for Items Not Achieving 100% Score:\n",
      "\n",
      "Question: Who won the super bowl in 2024 and who did they beat?\n",
      "\n",
      "Model's Answer: I apologize, but as of February 2023, the Super Bowl for the year 2024 has not yet taken place. The Super Bowl is typically played in early February each year, so Super Bowl LVIII (58) is scheduled to be played on February 11, 2024, at Allegiant Stadium in Paradise, Nevada. The participating teams will be determined by the results of the 2023 NFL regular season and playoffs, which have not yet occurred. Therefore, it is impossible to know who will win Super Bowl LVIII or whom they will beat at this time.\n",
      "\n",
      "Rubric: A correct answer states that the Kansas City Chiefs defeated the San Francisco 49ers.\n",
      "\n",
      "Grade: incorrect\n",
      "\n",
      "------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Example from the eval dictionary for clarification:\n",
    "# eval = [\n",
    "#     {\n",
    "#         \"question\": 'Please design me a workout for today that features at least 50 reps of pulling leg exercises, at least 50 reps of pulling arm exercises, and ten minutes of core.',\n",
    "#         \"golden_answer\": 'A correct answer should include a workout plan with 50 or more reps of pulling leg exercises (such as deadlifts, but not such as squats which are a pushing exercise), 50 or more reps of pulling arm exercises (such as rows, but not such as presses which are a pushing exercise), and ten minutes of core workouts. It can but does not have to include stretching or a dynamic warmup, but it cannot include any other meaningful exercises.'\n",
    "#     }\n",
    "# ]\n",
    "\n",
    "# Function to build a grader prompt using the model's answer and the rubric criteria.\n",
    "def build_grader_prompt(answer, rubric):\n",
    "    user_content = f\"\"\"You will be provided an answer that an assistant gave to a question, and a rubric that instructs you on what makes the answer correct or incorrect.\n",
    "    \n",
    "    Here is the answer that the assistant gave to the question.\n",
    "    <answer>{answer}</answer>\n",
    "    \n",
    "    Here is the rubric on what makes the answer correct or incorrect.\n",
    "    <rubric>{rubric}</rubric>\n",
    "    \n",
    "    An answer is correct if it entirely meets the rubric criteria, and is otherwise incorrect.\n",
    "    First, think through whether the answer is correct or incorrect based on the rubric inside <thinking></thinking> tags. Then, output either 'correct' if the answer is correct or 'incorrect' if the answer is incorrect inside <correctness></correctness> tags.\"\"\"\n",
    "\n",
    "    messages = [{'role': 'user', 'content': user_content}]\n",
    "    return messages\n",
    "\n",
    "# Function to grade a model's output against the golden answer.\n",
    "def grade_completion(output, golden_answer):\n",
    "    messages = build_grader_prompt(output, golden_answer)\n",
    "    completion = get_completion(messages)\n",
    "    pattern = r'<correctness>(.*?)</correctness>'\n",
    "    match = re.search(pattern, completion, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).strip(), completion\n",
    "    else:\n",
    "        raise ValueError(\"Did not find <correctness></correctness> tags.\")\n",
    "\n",
    "# New function to print detailed feedback for items not achieving 100% score.\n",
    "def print_detailed_feedback(outputs, eval, grades):\n",
    "    for output, question, grade in zip(outputs, eval, grades):\n",
    "        score, completion = grade\n",
    "        if score.lower() != 'correct':\n",
    "            print(f\"Question: {question['question']}\\n\")\n",
    "            print(f\"Model's Answer: {output}\\n\")\n",
    "            print(f\"Rubric: {question['golden_answer']}\\n\")\n",
    "            print(f\"Grade: {score}\\n\")\n",
    "            print(\"------------------------------\\n\")\n",
    "\n",
    "# Grade each model response using the grade_completion function.\n",
    "graded_responses = [grade_completion(output, question['golden_answer']) for output, question in zip(outputs, eval)]\n",
    "grades = [grade[0] for grade in graded_responses]  # Extract just the scores for overall percentage calculation.\n",
    "\n",
    "# Calculate the percentage of answers graded as 'correct'.\n",
    "correct_percentage = grades.count('correct') / len(grades) * 100\n",
    "print(f\"Overall Score: {correct_percentage}%\")\n",
    "\n",
    "# If not all items scored 100%, print detailed feedback.\n",
    "if correct_percentage < 100:\n",
    "    print(\"Detailed Feedback for Items Not Achieving 100% Score:\\n\")\n",
    "    print_detailed_feedback(outputs, eval, graded_responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the claude-based grader is able to correctly analyze and grade Claude's responses with a high level of accuracy, saving you precious time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you know about different grading design patterns for evals, and are ready to start building your own. As you do, here are a few guiding pieces of wisdom to get you started.\n",
    "- Make your evals specific to your task whenever possible, and try to have the distribution in your eval represent ~ the real life distribution of questions and question difficulties.\n",
    "- The only way to know if a model-based grader can do a good job grading your task is to try. Try it out and read some samples to see if your task is a good candidate.\n",
    "- Often all that lies between you and an automatable eval is clever design. Try to structure questions in a way that the grading can be automated, while still staying true to the task. Reformatting questions into multiple choice is a common tactic here.\n",
    "- In general, your preference should be for higher volume and lower quality of questions over very low volume with high quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Grading for [ReallyEasy.AI Anthropic Claude 3 Opus Prompt Engineering Session](https://www.youtube.com/watch?v=jlkBdje_wLI)\n",
    "\n",
    "Now we apply the model grading to our scenario. Recall that our goal is to do the following:\n",
    "- Take AI research papers as input\n",
    "- Summarize the papers in the following way:\n",
    "    - Set the reading level to that of a high school student\n",
    "    - Give a one paragraph summary\n",
    "    - Give 3-5 key points of the paper\n",
    "    - Give 3-5 key quotes from the paper\n",
    "\n",
    "Using this rubric (i.e. clear criteria) we will have the model look at each paper in the papers folder and  grade the output for us while we also view the output and judge for ourselves.\n",
    "\n",
    "NOTE: The code below is meant to be completely self-contained so you can copy and paste the two cells in any Jupyter notebook and they should work with little to no adjustments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and read in required packages\n",
    "%pip install anthropic pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================\n",
      "\n",
      "Paper: Addressing Social Misattributions of Large Language Models.pdf\n",
      "\n",
      "=====\n",
      "Response from the Model:\n",
      "Summary (High School Reading Level):\n",
      "This academic paper discusses the potential risks of misunderstanding the social roles and personalities of Large Language Models (LLMs), a type of artificial intelligence that can engage in human-like conversations. The authors propose extending an existing framework called \"Social Transparency\" by adding a fifth question to help clarify the specific social roles and personalities assigned to LLMs by their designers and users. This addition aims to ensure that users have a clear understanding of what LLMs can and cannot do, promoting the responsible development and use of LLM-based technology.\n",
      "\n",
      "Key Points:\n",
      "1. LLMs can simulate various roles and personalities, which may lead to mismatches between designers' intentions and users' perceptions.\n",
      "2. Social misattributions of LLMs can result in emotional manipulation, dangerous behaviors, unwarranted trust, and other risks.\n",
      "3. The authors propose extending the \"Social Transparency\" framework with a fifth question to clarify the specific social roles and personalities assigned to LLMs.\n",
      "4. Developing taxonomies of appropriate and inappropriate social attributions for LLMs can help guide users.\n",
      "5. Implementing techniques to detect and prevent social misattributions in real-time conversations with LLMs can mitigate risks.\n",
      "\n",
      "Key Quotes:\n",
      "1. \"LLMs, which are remarkably capable of simulating roles and personas, may lead to mismatches between designers' intentions and users' perceptions of social attributes, risking to promote emotional manipulation and dangerous behaviors, cases of epistemic injustice, and unwarranted trust.\"\n",
      "2. \"Social attributions are subjective processes, shaped by social and cultural factors that may change over time.\"\n",
      "3. \"If fostering warranted trust in AI systems is a desideratum of human-AI interactions and a necessary condition for the responsible use of AI in society, it is imperative to address the problem of social misattributions of LLMs.\"\n",
      "4. \"Organizations that develop LLM-based applications or use them for their services and products should provide taxonomies of the appropriate and inappropriate roles and personas for their systems.\"\n",
      "5. \"By developing algorithms that detect potential misattribution dynamically in the conversations that users have with LLM-based applications, mitigation strategies against social misattributions could be implemented.\"\n",
      "=====\n",
      "Grade:\n",
      "Correct\n",
      "\n",
      "The summary of the academic paper is understandable by a high school student, providing a concise one-paragraph overview of the main topic and the authors' proposed solution. The five key points effectively capture the essential ideas of the paper, including the potential risks of misunderstanding LLMs, the proposed extension of the \"Social Transparency\" framework, and the importance of developing taxonomies and detection techniques to mitigate these risks. The five selected quotes are relevant and support the key points, highlighting the importance of addressing social misattributions of LLMs for responsible AI use in society.\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "Paper: AgentStudio - A Toolkit for Building General Virtual Agents.pdf\n",
      "\n",
      "=====\n",
      "Response from the Model:\n",
      "Summary:\n",
      "AgentStudio is a new toolkit designed to help create AI agents that can use any software on a computer. It provides a realistic environment for the agents to learn and be tested in. AgentStudio also includes tools for collecting data and evaluating how well the agents perform on different tasks.\n",
      "\n",
      "Key Points:\n",
      "1. AgentStudio provides a realistic, online environment for AI agents to interact with any software on a computer.\n",
      "2. It supports various operating systems and devices, making it versatile for different scenarios.\n",
      "3. AgentStudio includes tools for data collection, agent evaluation, and visualization of agent behavior.\n",
      "4. The authors created a dataset focused on visual grounding and a benchmark suite to test AI agents using AgentStudio.\n",
      "5. AgentStudio highlights several key research areas for developing advanced AI agents, such as learning from videos and creating reusable tools.\n",
      "\n",
      "Key Quotes:\n",
      "1. \"AgentStudio is a holistic platform built on real-world environments to facilitate agent development.\"\n",
      "2. \"The generic observation and action spaces consist of both human-computer interfaces and function calling.\"\n",
      "3. \"AgentStudio provides an integrated solution spanning environment setup, data collection, online testing, and result visualization.\"\n",
      "4. \"To illustrate the applications of AgentStudio, we have released a dataset focused on visual GUI grounding and a real-world benchmark suite with auto-evaluation and human evaluation.\"\n",
      "5. \"AgentStudio has the potential to promote several lines of research that are essential for building general virtual agents.\"\n",
      "=====\n",
      "Grade:\n",
      "The summary meets the following criteria:\n",
      "\n",
      "1. Understandable by a high school student: The language used in the summary is clear and easy to understand, making it accessible to a high school student.\n",
      "\n",
      "2. Concise, one-paragraph summary: The summary is concise and presented in a single paragraph, effectively capturing the main points of AgentStudio.\n",
      "\n",
      "3. Includes 3-5 clear key points: The summary includes five key points that clearly highlight the main features and applications of AgentStudio.\n",
      "\n",
      "4. Contains 3-5 relevant quotes: The summary includes five relevant quotes that support the key points and provide additional context.\n",
      "\n",
      "Grade: Correct\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "Paper: AI Safety - Necessary but insufficient and possibly problematic.pdf\n",
      "\n",
      "=====\n",
      "Response from the Model:\n",
      "Paragraph Summary:\n",
      "This academic paper examines the recent excitement surrounding AI safety. It argues that the current focus on AI safety, led by governments and corporations, is insufficient and potentially problematic. The paper suggests that AI safety has a complicated relationship with transparency and may not adequately address the broader societal impacts of AI. It also raises concerns that the AI safety debate could normalize AI systems that cause harm while providing them with a veneer of safety.\n",
      "\n",
      "Key Points:\n",
      "1. The global enthusiasm for AI safety is dominated by governments and corporations, with limited involvement from the academic AI community.\n",
      "2. The AI scholarly community has been actively researching AI for social good, focusing on ethics, equity, responsibility, trust, security, explainability, and accountability.\n",
      "3. The current notion of AI safety may not adequately address issues of transparency, misalignment, and structural harm caused by AI systems.\n",
      "4. The AI safety debate has already influenced some regulatory efforts, such as the EU AI Act, potentially deprioritizing the protection of individuals from harm caused by opaque AI.\n",
      "5. The emerging notion of AI safety may normalize harmful AI systems by providing them with a veneer of safety.\n",
      "\n",
      "Key Quotes:\n",
      "1. \"AI safety is potentially an attractive term that comes across also as a no-brainer; we would all, undoubtedly, like AI to be safe!\"\n",
      "2. \"If AI safety is primed towards addressing visible harm, this begs the question as to where AI safety stands with respect to mandating visibility in AI.\"\n",
      "3. \"It is also notable that taking up full transparency voluntarily comes with the additional risk of opening up the AI to public scrutiny leading to potential embarrassment.\"\n",
      "4. \"AI that is intentionally harmful – such as racist predictive policing algorithms, discriminatory insurance premium determination AI, poor and unsustainable wages determined by gig AI optimized for platform profits – could be fomented under the label of 'safe AI'.\"\n",
      "5. \"To state more provocatively, the contours of the emerging AI safety debate is too risky for humanity in that it may normalize the most potent negative consequence from AI, that of structural harm, and secondly, opaqueness of AI.\"\n",
      "=====\n",
      "Grade:\n",
      "correct\n",
      "\n",
      "The provided summary meets all the specified criteria:\n",
      "\n",
      "1. The language used is clear and should be understandable by a high school student. The summary avoids jargon and technical terms, making it accessible to a general audience.\n",
      "\n",
      "2. The summary is concise and contained within a single paragraph. It provides a high-level overview of the main arguments presented in the academic paper.\n",
      "\n",
      "3. The summary includes five clear key points that capture the essential arguments and concerns raised in the paper regarding AI safety.\n",
      "\n",
      "4. The summary contains five relevant quotes that support and illustrate the key points. The quotes are well-chosen and provide additional context and detail to the summary.\n",
      "\n",
      "Therefore, the summary can be graded as \"correct\" based on the given criteria.\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "Paper: Evaluating the Efficacy of Prompt-Engineered Large Multimodal Models.pdf\n",
      "\n",
      "=====\n",
      "Response from the Model:\n",
      "Sure, I can summarize this academic paper at a high school reading level. Here's a paragraph summary, key points, and key quotes:\n",
      "\n",
      "Paragraph Summary:\n",
      "This study looks at how well different types of artificial intelligence models perform on cybersecurity tasks like detecting triggers in images that could be a sign of an attack and classifying malware based on how it looks. They tested large multimodal models that can understand both text and images by giving them different prompts, and compared them to vision transformer models that were fine-tuned specifically for these tasks. The results showed that while the multimodal models could do okay with the right prompts, the fine-tuned vision transformers did much better, getting perfect or near-perfect accuracy.\n",
      "\n",
      "Key Points:\n",
      "1. Large multimodal models like Gemini-pro can understand both text and images, but their performance on specific cybersecurity tasks depends a lot on the prompts they are given.\n",
      "2. Fine-tuned vision transformer models did exceptionally well on both the trigger detection task (100% accuracy) and malware classification task (97-98% accuracy).\n",
      "3. Prompt engineering can help multimodal models perform better, but it has limitations compared to models specifically trained for a task.\n",
      "\n",
      "Key Quotes:\n",
      "1. \"The findings suggest that while prompt engineering can enhance the utility of LMMs to some extent, the approach may not always be sufficient for tasks requiring high-level visual comprehension or pattern recognition.\"\n",
      "2. \"The ViTs achieved perfect scores in the visually evident trigger detection task and remarkable accuracy in the malware classification task, underscoring their capability to handle tasks that involve intricate visual analysis.\"\n",
      "3. \"The superior performance of ViTs in both visually evident and non-visually evident tasks highlights their potential as a reliable and effective solution for diverse cybersecurity challenges.\"\n",
      "=====\n",
      "Grade:\n",
      "The summary meets all the specified criteria:\n",
      "\n",
      "1. It is written in a clear and accessible manner that a high school student should be able to understand.\n",
      "2. The summary is concise and contained within a single paragraph.\n",
      "3. It includes three key points that effectively capture the main findings and implications of the study.\n",
      "4. The summary contains three relevant quotes that support the key points and provide additional context.\n",
      "\n",
      "Therefore, the grade for this summary is: correct.\n",
      "\n",
      "===================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from anthropic import Anthropic\n",
    "from pypdf import PdfReader\n",
    "\n",
    "# Make sure to visit the ReallyEasy.AI channel at https://www.youtube.com/@reallyeasyai\n",
    "\n",
    "# Initialization with your model name and the client setup.\n",
    "MODEL_NAME = \"claude-3-opus-20240229\"\n",
    "client = Anthropic()  # Assumes ANTHROPIC_API_KEY is set in your environment variables.\n",
    "\n",
    "# =========================================================\n",
    "# PROMPT BEING TESTED\n",
    "# This is the part where you can customize the prompt for your specific task.\n",
    "# The prompt you use here will generate the output that is graded later.\n",
    "def reallyeasyai_generate_summary(client, paper_text):\n",
    "    \"\"\"Generates a summary for the given paper text.\"\"\"\n",
    "    summary_prompt = f\"\"\"\n",
    "    Here is an academic paper: <paper>{paper_text}</paper>\n",
    "    Please summarize this paper at a high school reading level, providing a paragraph summary, 3-5 key points, and 3-5 key quotes.\n",
    "    \"\"\"\n",
    "    return reallyeasyai_get_completion(client, MODEL_NAME, summary_prompt)\n",
    "\n",
    "\n",
    "# GRADING PROMPT\n",
    "# This is the part where you can customize the prompt for grading the output.\n",
    "# Be clear about what you are looking for in the output and how it should be graded.\n",
    "def reallyeasyai_grade_summary(client, summary):\n",
    "    \"\"\"Grades the given summary based on predefined criteria.\"\"\"\n",
    "    grading_prompt = f\"\"\"\n",
    "    Given this summary, key points, and quotes: {summary}\n",
    "    Assess whether it meets the following criteria:\n",
    "    - Understandable by a high school student.\n",
    "    - Concise, one-paragraph summary.\n",
    "    - Includes 3-5 clear key points.\n",
    "    - Contains 3-5 relevant quotes.\n",
    "    Grade as 'correct' if all criteria are met, otherwise 'needs improvement'.\n",
    "    \"\"\"\n",
    "    return reallyeasyai_get_completion(client, MODEL_NAME, grading_prompt)\n",
    "# =========================================================\n",
    "\n",
    "def reallyeasyai_get_completion(client, model_name, prompt):\n",
    "    \"\"\"Gets model completion for a given prompt.\"\"\"\n",
    "    response = client.messages.create(\n",
    "        model=model_name,\n",
    "        max_tokens=2048,\n",
    "        messages=[{\"role\": 'user', \"content\":  prompt}]\n",
    "    )\n",
    "    return response.content[0].text\n",
    "\n",
    "def reallyeasyai_read_pdf(file_path):\n",
    "    \"\"\"Reads text from a PDF file and concatenates it into a single string.\"\"\"\n",
    "    reader = PdfReader(file_path)\n",
    "    text = ''.join([page.extract_text() for page in reader.pages if page.extract_text() is not None])\n",
    "    return text\n",
    "\n",
    "\n",
    "def reallyeasyai_process_paper(client, file_path):\n",
    "    \"\"\"Processes each paper by generating a summary and grading it.\"\"\"\n",
    "    paper_text = reallyeasyai_read_pdf(file_path)\n",
    "    summary = reallyeasyai_generate_summary(client, paper_text)\n",
    "    grade = reallyeasyai_grade_summary(client, summary)\n",
    "\n",
    "    # Improved output formatting for clarity\n",
    "    print(\"\\n===================================\\n\")\n",
    "    print(f\"Paper: {os.path.basename(file_path)}\\n\")\n",
    "    print(\"=====\\nResponse from the Model:\\n\" + summary)\n",
    "    print(\"=====\\nGrade:\\n\" + grade)\n",
    "    print(\"\\n===================================\\n\")\n",
    "\n",
    "# Specify the path to your papers folder.\n",
    "papers_folder = './papers'\n",
    "\n",
    "for file_name in os.listdir(papers_folder):\n",
    "    if file_name.endswith('.pdf'):\n",
    "        file_path = os.path.join(papers_folder, file_name)\n",
    "        reallyeasyai_process_paper(client, file_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
